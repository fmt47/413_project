{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resnet152 model for Covid-19 classification"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, models\n",
    "\n",
    "from PIL import Image\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_time(elapsed):\n",
    "    elapsed_rounded = int(round(elapsed, 2))\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class COVIDDataset(Dataset):\n",
    "    def __init__(self, data_file, transform=None):\n",
    "        self.data = []\n",
    "        self.transform = transform\n",
    "        self.map = {'negative': 0, 'positive': 1}\n",
    "        \n",
    "        with open(data_file, 'r') as f:\n",
    "            folder = os.path.splitext(data_file)[0]\n",
    "            for line in f:\n",
    "                patient_id, filename, label, data_source = line.strip().split(' ')\n",
    "                image_path = os.path.join(folder, filename)\n",
    "                self.data.append((image_path ,self.map[label]))\n",
    "                \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        image_path, label = self.data[index]\n",
    "        image = Image.open(image_path)\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "        return image, label"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the modified ResNet152 model\n",
    "class ModifiedResNet152(nn.Module):\n",
    "    def __init__(self, num_classes=2, pretrained=True):\n",
    "        super(ModifiedResNet152, self).__init__()\n",
    "        if pretrained:\n",
    "            self.resnet = models.resnet152(weights=models.ResNet152_Weights.DEFAULT)\n",
    "        else:\n",
    "            self.resnet = models.resnet152()\n",
    "        self.resnet.avgpool = nn.AdaptiveAvgPool2d(output_size=(1, 1))\n",
    "        self.resnet.fc = nn.Sequential(\n",
    "            nn.Linear(in_features=2048, out_features=1024),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(in_features=1024, out_features=num_classes),\n",
    "            nn.LogSoftmax(dim=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.resnet(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "def train(model, train_loader, optimizer, criterion, device, mse=False):\n",
    "    model.train()\n",
    "    train_loss = 0.\n",
    "    correct = 0.\n",
    "    total = 0.\n",
    "    print(f'Loading # {len(train_loader)} datas')\n",
    "    \n",
    "    with tqdm(train_loader, unit='batch') as tepoch:\n",
    "        for data, target in tepoch:\n",
    "            tepoch.set_description(f\"Training\")\n",
    "            data, target = data.to(device), target.to(device)\n",
    "\n",
    "            # instead of  optimizer.zero_grad(), do it in forloop for more efficiency\n",
    "            for param in model.parameters():\n",
    "                param.grad = None\n",
    "\n",
    "            output = model(data)\n",
    "            if not mse:\n",
    "                loss = criterion(output, target)\n",
    "            else:\n",
    "                target_one_hot = torch.zeros(target.shape[0], 2).to(device)\n",
    "                target_one_hot.scatter_(1, target.unsqueeze(1), 1)\n",
    "                loss = criterion(output, target_one_hot.to(device))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            _, predicted = output.max(1)\n",
    "            total += target.size(0)\n",
    "            correct += predicted.eq(target).sum().item()\n",
    "        \n",
    "    acc = 100.*correct/total\n",
    "    return train_loss, acc\n",
    "\n",
    "def validate(model, val_loader, criterion, device, mse=False):\n",
    "    model.eval()\n",
    "    val_loss = 0.\n",
    "    correct = 0.\n",
    "    total = 0.\n",
    "    false_positive = 0\n",
    "    false_negative = 0\n",
    "    true_positive = 0\n",
    "    true_negative = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        with tqdm(val_loader, unit='batch') as tepoch:\n",
    "            for data, target in tepoch:\n",
    "                tepoch.set_description(f\"Validating\")\n",
    "                data, target = data.to(device), target.to(device)\n",
    "                output = model(data)\n",
    "                if not mse:\n",
    "                    val_loss += criterion(output, target).item()\n",
    "                else:\n",
    "                    target_one_hot = torch.zeros(target.shape[0], 2).to(device)\n",
    "                    target_one_hot.scatter_(1, target.unsqueeze(1), 1)\n",
    "                    val_loss += criterion(output, target_one_hot.to(device))\n",
    "                _, predicted = output.max(1)\n",
    "                total += target.size(0)\n",
    "                false_positive += ((predicted == 1) & (target == 0)).sum().item()\n",
    "                false_negative += ((predicted == 0) & (target == 1)).sum().item()\n",
    "                true_negative += ((predicted == 0) & (target == 0)).sum().item()\n",
    "                true_positive += ((predicted == 1) & (target == 1)).sum().item()\n",
    "    correct = true_positive + true_negative\n",
    "    sensitivity = true_positive / (true_positive + false_negative)\n",
    "    specificity = true_negative / (true_negative + false_positive)\n",
    "    f1_score = 2 * true_positive / (2 * true_positive + false_positive + false_negative)\n",
    "    print(f'Validation loss: {val_loss:.4f}, Accuracy: {correct}/{total} ({100.*correct/total:.2f}%), Sensitivity: {sensitivity:.2f}, Specificity: {specificity:.2f}, F1 score: {f1_score:.2f}')\n",
    "    acc = 100.*correct/total\n",
    "    return val_loss, acc, sensitivity, specificity, f1_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_resnet(model, dataset, lr, epochs, batch_size, factor, patience, criterion, device, pretrain, state_for='unprovided', mse=False):\n",
    "    \"\"\"Train the model on the dataset and return the best accuracy\n",
    "    \n",
    "    Args:\n",
    "        model: the model to train\n",
    "        dataset: the dataset to train on\n",
    "        lr: learning rate\n",
    "        epochs: number of epochs to train for\n",
    "        batch_size: batch size\n",
    "        factor: factor for ReduceLROnPlateau\n",
    "        patience: patience for ReduceLROnPlateau\n",
    "        criterion: loss function\n",
    "        device: device to train on (cuda or cpu)\n",
    "        \n",
    "    Returns:\n",
    "        best_acc: best accuracy achieved\n",
    "    \"\"\"\n",
    "    PRETRAIN = 'finetune' if pretrain else 'wo_pretrain'\n",
    "    print(f\"Using device: {device}\")\n",
    "    # get size of dataset\n",
    "    dataset_size = len(dataset)\n",
    "    print(f\"Dataset size: {dataset_size}\")\n",
    "    # split dataset into train and validation sets\n",
    "    train_size = int(0.8 * dataset_size)\n",
    "    val_size = dataset_size - train_size\n",
    "    train_dataset, val_dataset = data.random_split(dataset, [train_size, val_size])\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size,\n",
    "                            shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size,\n",
    "                            shuffle=False)\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=factor, patience=patience, verbose=True)\n",
    "    # record training and validation losses and accuracies\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    train_accs = []\n",
    "    val_accs = []\n",
    "    best_acc = 0\n",
    "    best_epoch = 0\n",
    "    model_state_path = \"\"\n",
    "\n",
    "    start = time.time()\n",
    "    print(f\"Began training for {epochs} epochs\")\n",
    "    for epoch in range(epochs):\n",
    "        print(f'Training (Epoch {epoch + 1})')\n",
    "        torch.cuda.empty_cache()\n",
    "        t0 = time.time()\n",
    "        train_loss, train_acc = train(model, train_loader, optimizer, criterion, device, mse)\n",
    "        print(\"  * Training epoch took: {:}\".format(format_time(time.time() - t0)))\n",
    "        \n",
    "        t0 = time.time()\n",
    "        val_loss, val_acc, val_sen, val_spec, val_f1 = validate(model, val_loader, criterion, device, mse)\n",
    "        print(\"  * Validation took: {:}\".format(format_time(time.time() - t0)))\n",
    "\n",
    "        # record training and validation losses and accuracies\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        train_accs.append(train_acc)\n",
    "        val_accs.append(val_acc)\n",
    "        \n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "        print(f'Epoch {epoch + 1}, Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.3f}\\nVal Sensitivity: {val_sen:.3f}, Val Specificity: {val_spec:.3f}, Val F1 score: {val_f1:.3f}')\n",
    "        \n",
    "        if val_acc > best_acc:\n",
    "            best_acc = val_acc\n",
    "            best_epoch = epoch + 1\n",
    "            model_state_path = f'{state_for}_{PRETRAIN}_resnet152.pt'\n",
    "            torch.save(model.state_dict(), model_state_path)\n",
    "    \n",
    "    print(\"Training complete in {}\".format(format_time(time.time() - start)))\n",
    "    print(\"Best validation accuracy: {:.2f}% -- Acquired with epoch {}\".format(best_acc, best_epoch))\n",
    "\n",
    "    return train_losses, val_losses, train_accs, val_accs, model_state_path"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Begin Training process for RESNET152"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up hyperparameters for the model\n",
    "# Set up hyperparameters and data loaders\n",
    "# TUNABLE:\n",
    "lr = 2e-4\n",
    "# TUNABLE:\n",
    "epochs = 10\n",
    "# TUNABLE: original 64\n",
    "batch_size = 16\n",
    "# TUNABLE:\n",
    "factor = 0.7\n",
    "# TUNABLE:\n",
    "patience = 5\n",
    "# set the model with pretrained weight or not\n",
    "pretrain = True \n",
    "\n",
    "num_classes = 2\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# # Define the transformations to be applied to the data\n",
    "target_size = 224\n",
    "transform = transforms.Compose([\n",
    "        transforms.Grayscale(num_output_channels=3),\n",
    "        transforms.Resize(target_size),\n",
    "        transforms.CenterCrop(target_size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "# create COVIDDataset object\n",
    "# TUNABLE: --> can try different datasets\n",
    "dataset = COVIDDataset('train.txt', transform=transform)\n",
    "\n",
    "# Define the model, loss function\n",
    "model = ModifiedResNet152(num_classes, pretrained=pretrain).to(device) # pretrained=True\n",
    "\n",
    "# TUNABLE: Choose the loss function and optimizer.\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "state_for = 'ADAM+CE'\n",
    "\n",
    "# Remember to add a parameter to the train function to indicate whether to use MSE or not\n",
    "# criterion = nn.functional.mse_loss\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "# state_for = 'ADAM+MSE'\n",
    "\n",
    "# criterion = nn.NLLLoss()\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "# state_for = 'ADAM+NLL'\n",
    "\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "# state_for = 'SGD'\n",
    "\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "# optimizer = torch.optim.RMSprop(model.parameters(), lr=lr)\n",
    "# state_for = 'RMSPROP'\n",
    "\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "# optimizer = torch.optim.Adadelta(model.parameters(), lr=1.0, rho=0.9, eps=1e-06, weight_decay=0)\n",
    "# state_for = 'ADADelta'\n",
    "\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "# optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "# state_for = 'AdamW'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "train_losses, val_losses, train_accs, val_accs, model_state_path = train_resnet(model, dataset, lr, epochs, batch_size, factor, patience, criterion, device, pretrain, state_for)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the loss and accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot training and validation losses and accuracies\n",
    "now = datetime.datetime.now()\n",
    "now = now.strftime(\"%Y-%m-%d_%H%M%S\")\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title(\"Loss (ResNet152)\")\n",
    "plt.plot(train_losses, label='Training loss')\n",
    "plt.plot(val_losses, label='Validation loss')\n",
    "plt.legend()\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title(\"Accuracy (ResNet152)\")\n",
    "plt.plot(train_accs, label='Training accuracy')\n",
    "plt.plot(val_accs, label='Validation accuracy')\n",
    "plt.legend()\n",
    "plt.savefig('./resnet_plots/' + f'{state_for}_loss_resnet152_{now}.png')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the resnet model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test data\n",
    "test_dataset = COVIDDataset('test.txt', transform=transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "model_path = model_state_path\n",
    "# model_path = 'RMSPROP_finetune_resnet152.pt'\n",
    "\n",
    "# load best model\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "print(f'Model {model_path} loaded')\n",
    "# evaluate model on test set\n",
    "test_loss, test_acc, test_sen, test_spec, test_f1 = validate(model, test_loader, criterion, device)\n",
    "print(f'Test loss: {test_loss:.4f}, Test accuracy: {test_acc:.3f}%, Test sensitivity: {test_sen:.3f}%, Test specificity: {test_spec:.3f}%, Test F1 score: {test_f1:.3f}%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "csc413proj_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
